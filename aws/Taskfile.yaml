# https://taskfile.dev
# Taskfile for automating AWS EKS, ArgoCD deployment and destruction

version: '3'


vars:
  GREETING: "Time to deploy some EKS Clusters the GlueOps way!"
env:
  TF_VAR_COMPANY_KEY: 
    sh: echo -n $COMPANY_KEY
  TF_VAR_ADMIRAL_ACCOUNT_ID:
    sh: echo -n $(aws organizations list-accounts | jq '.Accounts | .[] | select(.Name | contains("admiral")) | .Id')
  TF_VAR_CAPTAIN_ACCOUNT_ID:
    sh: echo -n $(aws organizations list-accounts | jq '.Accounts | .[] | select(.Name | contains("captain")) | .Id')
  AWS_ACCESS_KEY_ID:
    sh: echo -n $AWS_ACCESS_KEY_ID
  AWS_SECRET_ACCESS_KEY:
    sh: echo -n $AWS_SECRET_ACCESS_KEY
  AWS_REGION:
    sh: echo -n $AWS_REGION
  SVC_LB_PATCH:
    sh: echo -n '{"spec"'':' '{"type"'':'  '"LoadBalancer"}}'
  


tasks:
  default:
    desc: Shows a list of all tasks
    cmds:
      - task -a

  test:
    cmds:
      - echo {{.GREETING}}
  
  configs:
    desc: Generate configs
    cmds:
      - envsubst < ../shared/application-definition-for-apps-cluster.yaml.tpl > ../shared/application-definition-for-apps-cluster.yaml

  eks_up:
    desc: Deploy Admiral and Captain AWS EKS clusters
    cmds:
      - date >> run.txt
      - terraform init
      - terraform apply --auto-approve
      - terraform apply
      - date >> run.txt


#- read ak sak st < <(echo $(aws sts assume-role --role-arn "arn:aws:iam::539362929792:role/OrganizationAccountAccessRole" --role-session-name k8s-test | jq -r '.Credentials | "\(.AccessKeyId) \(.SecretAccessKey) \(.SessionToken)"')) && export AWS_ACCESS_KEY_ID=$ak AWS_SECRET_ACCESS_KEY=$sak AWS_SESSION_TOKEN=$st



  clean:
    desc: Shutdown GCP Projects that contain the GKE clusters.
    vars:
      FOLDER_ID:
        sh: gcloud resource-manager folders list --organization=`gcloud organizations list --format=json | jq -r '.[0].name | split("/") | .[1]'` --filter="{{.TF_VAR_COMPANY_KEY}} Core" --format=json | jq -r '.[0].name | split("/") | .[1]' || true
    cmds:
      - gcloud projects delete {{.TF_VAR_COMPANY_KEY}}-apps-{{.TF_VAR_TEST_NUMBER}} --quiet || true
      - gcloud projects delete {{.TF_VAR_COMPANY_KEY}}-admiral-{{.TF_VAR_TEST_NUMBER}} --quiet || true
      - gcloud alpha billing projects unlink {{.TF_VAR_COMPANY_KEY}}-apps-{{.TF_VAR_TEST_NUMBER}} || true
      - gcloud alpha billing projects unlink {{.TF_VAR_COMPANY_KEY}}-admiral-{{.TF_VAR_TEST_NUMBER}} || true
      - gcloud resource-manager folders update {{.FOLDER_ID}} --display-name=`date +%s`"-DELETED" || true
      - rm -rf *terraform*
      - rm -rf .terraform*
      - rm -rf ~/.kube/
      - rm -rf credentials.txt

  gke_bootstrap_argocd:
    desc: install argocd on admiral cluster
    cmds:
      - rm -rf ~/.kube/
      - gcloud container clusters get-credentials admiral-{{.TF_VAR_TEST_NUMBER}}-gke --zone us-central1-a --project {{.TF_VAR_COMPANY_KEY}}-admiral-{{.TF_VAR_TEST_NUMBER}}
      - kubectl config rename-context `kubectl config current-context` {{.TEST_VAR}} {{.TF_VAR_COMPANY_KEY}}-{{.TF_VAR_TEST_NUMBER}}-admiral
      - mv ~/.kube/config ~/.kube/admiral
      - gcloud container clusters get-credentials apps-{{.TF_VAR_TEST_NUMBER}}-gke --zone us-central1-a --project {{.TF_VAR_COMPANY_KEY}}-apps-{{.TF_VAR_TEST_NUMBER}}
      - kubectl config rename-context `kubectl config current-context` {{.TEST_VAR}} {{.APPS_CLUSTER_NAME}}
      - mv ~/.kube/config ~/.kube/apps
      - KUBECONFIG={{.HOME}}/.kube/admiral:{{.HOME}}/.kube/apps kubectl config view --flatten > /tmp/config && mv /tmp/config ~/.kube/config
      - rm ~/.kube/admiral ~/.kube/apps
      - kubectl config use-context {{.TF_VAR_COMPANY_KEY}}-{{.TF_VAR_TEST_NUMBER}}-admiral
      - kubectl create namespace argocd || true
      - kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/v2.4.11/manifests/install.yaml
      - kubectl wait pods --all -n argocd --for condition=Ready --timeout=120s
      - kubectl patch svc argocd-server -n argocd -p '{"spec"'':' '{"type"'':'  '"LoadBalancer"}}'
      # this is structured differently in AWS response so, we'll need to edit this for aws
      #GCP
      #- until argocd login $(kubectl get service argocd-server -n argocd --output=jsonpath="{.status.loadBalancer.ingress[0].ip}") --username admin --password $(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d; echo) --grpc-web --insecure; do echo "" ; done
      #AWS
      - until argocd login $(kubectl get service argocd-server -n argocd --output=jsonpath="{.status.loadBalancer.ingress[0].hostname}") --username admin --password $(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d; echo) --grpc-web --insecure; do echo "" ; done
      ## For AWS you need to be in the context of the captain cluster. So basically assume the role for the captain cluster but be logged into argo on the admiral cluster
      - argocd cluster add {{.APPS_CLUSTER_NAME}} --grpc-web --yes
      - kubectl apply -f admiral-argocd-health-check.yaml -n argocd

  get_argocd_logins:
    desc: get logins for argocd
    cmds:
      - rm -rf credentials.txt
      - kubectl config use-context {{.TF_VAR_COMPANY_KEY}}-{{.TF_VAR_TEST_NUMBER}}-admiral
      #GCP
      #- echo 'Admiral Server:' 'https://'$(kubectl get service argocd-server -n argocd --output=jsonpath="{.status.loadBalancer.ingress[0].ip}") >> credentials.txt && echo 'User:' 'admin' >> credentials.txt  && echo 'Password:' $(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d) >> credentials.txt
      #AWS
      - echo 'Admiral Server:' 'https://'$(kubectl get service argocd-server -n argocd --output=jsonpath="{.status.loadBalancer.ingress[0].hostname}") >> credentials.txt && echo 'User:' 'admin' >> credentials.txt  && echo 'Password:' $(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d) >> credentials.txt
      - kubectl config use-context {{.APPS_CLUSTER_NAME}}
      - echo 'Apps Server:' 'https://'$(kubectl get ing -n glueops-core -o=jsonpath="{$.items[0].spec.rules[0].host}") >> credentials.txt && echo 'User:' 'admin' >> credentials.txt  && echo 'Password:' $(kubectl -n glueops-core get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d) >> credentials.txt

  gke_bootstrap_apps_cluster:
    desc: Using the ADMIRAL, install an argo app to bootstrap the apps cluster
    cmds:
      - kubectl config use-context {{.TF_VAR_COMPANY_KEY}}-{{.TF_VAR_TEST_NUMBER}}-admiral
      - kubectl apply -f application-definition-for-apps-cluster.yaml -n argocd



### aws_auth
## Assume role
# aws sts assume-role --role-arn "arn:aws:iam::041618144804:role/OrganizationAccountAccessRole" --role-session-name k8s-test

## get creds from responds, jq, export variables like this:
# export AWS_ACCESS_KEY_ID=""
# export AWS_SECRET_ACCESS_KEY=""
# export AWS_SESSION_TOKEN=""

## updatedate ~/.kube/config
# aws eks update-kubeconfig --region us-west-2 --name test-nonprod-test-stage-test-name-cluster
